{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c-8xfukYvhy"
      },
      "source": [
        "# ProteinTTT â€” Google Colab\n",
        "### Customize ESMFold with Test-Time Training for Enhanced Structure Prediction\n",
        "\n",
        "ðŸ“„ [Paper](https://arxiv.org/abs/2411.02109) | ðŸ’» [GitHub](https://github.com/anton-bushuiev/ProteinTTT)\n",
        "\n",
        "**How to use:** Edit the **sequence & config** cell below, then **Runtime â†’ Run all**. Results appear at the bottom.\n",
        "\n",
        "> âš ï¸ Go to **Runtime â†’ Change runtime type â†’ GPU** (T4 or better) before running."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDe3M1M_Yvh1"
      },
      "source": [
        "---\n",
        "## âœï¸ Input â€” edit this cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6q5F4jO3Yvh1"
      },
      "outputs": [],
      "source": [
        "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "# â•‘  EDIT YOUR SEQUENCE AND SETTINGS HERE, THEN \"Runtime â†’ Run all\"           â•‘\n",
        "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "SEQUENCE = (\n",
        "    \"GIHLGELGLLPSTVLAIGYFENLVNIICESLNMLPKLEVSGKEYKKFKFTIVIPKDLDANIKKRAKIYFKQKSLIEIEIPTSSRNYPIHIQFDENSTDDILHLYDMPTTIGGIDKAIEMFMRKGHIGKTDQQKLLEERELRNFKTTLENLIATDAFAKEMVEVIIEE\"\n",
        ")\n",
        "\n",
        "# ProteinTTT hyper-parameters\n",
        "TTT_STEPS     = 5     # number of TTT optimisation steps  (5â€“30)\n",
        "LEARNING_RATE = 4e-4   # step size for parameter updates\n",
        "BATCH_SIZE    = 4      # samples per TTT batch\n",
        "LORA_RANK     = 8      # rank of LoRA adapter matrices\n",
        "LORA_ALPHA    = 32     # scaling factor for LoRA\n",
        "\n",
        "# Where to save PDB files (Colab /content is writable)\n",
        "OUTPUT_DIR = \"/content/proteinttt_results\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaf4WVrGYvh2"
      },
      "source": [
        "---\n",
        "*Everything below runs automatically â€” no edits needed.*\n",
        "## 1 Â· Check GPU & validate input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0H-I0VWEYvh3",
        "outputId": "bb8cb0ce-ee25-421b-d368-481210deac17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… GPU: Tesla T4 (14.6 GB)\n",
            "âœ… Sequence accepted: 167 residues\n",
            "âœ… Output directory: /content/proteinttt_results\n"
          ]
        }
      ],
      "source": [
        "import torch, os\n",
        "\n",
        "# GPU check\n",
        "if not torch.cuda.is_available():\n",
        "    raise RuntimeError(\"No GPU detected! Go to Runtime â†’ Change runtime type â†’ GPU.\")\n",
        "gpu_name = torch.cuda.get_device_name(0)\n",
        "gpu_mem  = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "DEVICE   = \"cuda\"\n",
        "print(f\"âœ… GPU: {gpu_name} ({gpu_mem:.1f} GB)\")\n",
        "\n",
        "# Validate sequence\n",
        "_seq = SEQUENCE.strip().upper()\n",
        "_valid_aa = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
        "assert len(_seq) >= 10,                        f\"Sequence too short ({len(_seq)} aa). Need â‰¥ 10.\"\n",
        "assert all(c in _valid_aa for c in _seq),       \"Sequence contains invalid characters. Use standard 20 amino acids.\"\n",
        "assert len(_seq) <= 400,                        f\"Sequence too long ({len(_seq)} aa). Max 400 for this demo.\"\n",
        "SEQUENCE = _seq\n",
        "print(f\"âœ… Sequence accepted: {len(SEQUENCE)} residues\")\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"âœ… Output directory: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av1rqVNJYvh3"
      },
      "source": [
        "## 2 Â· Install dependencies (takes ~5 min on first run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCV27MjMYvh3",
        "outputId": "9fefdee0-5364-47f2-cd0b-7c156e800249"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  âœ… numpy 2.0.2 C extensions locked in memory\n",
            "\u001b[31mERROR: Cannot install biopython==1.85, biotite==0.40.0 and numpy==2.0.2 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for proteinttt (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  âš ï¸ esm package not found â€” skipping dataclass patches\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pytorch_lightning'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1430559363.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Verify critical packages imported correctly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorchmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m print(f\"âœ… pip packages installed \"\n\u001b[1;32m     58\u001b[0m       \u001b[0;34mf\"(pytorch-lightning {pytorch_lightning.__version__}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_lightning'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# â”€â”€ Pre-load numpy C extensions BEFORE pip can change files on disk. â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# numpy lazy-loads submodules (random, fft, linalg). If pip replaces numpy\n",
        "# on disk mid-session, the lazily-loaded .so files won't match the already-\n",
        "# loaded core â†’ ABI crash.  Eagerly importing them here locks everything in\n",
        "# memory from the SAME consistent build.\n",
        "import numpy, numpy.random, numpy.fft, numpy.linalg\n",
        "_NUMPY_VER = numpy.__version__\n",
        "print(f\"  âœ… numpy {_NUMPY_VER} C extensions locked in memory\")\n",
        "\n",
        "# â”€â”€ Pin numpy to the EXACT in-memory version so pip cannot change it â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "!pip install -q \\\n",
        "    \"numpy=={_NUMPY_VER}\" \\\n",
        "    biopython==1.85 \\\n",
        "    fair-esm omegaconf==2.3.0 \\\n",
        "    biotite==0.40.0 modelcif einops==0.8.1 \\\n",
        "    ml-collections dm-tree \\\n",
        "    \"pytorch-lightning>=1.8.4,<2.5\" \\\n",
        "    \"torchmetrics>=0.10.3\" \\\n",
        "    \"transformers>=4.30.0\" \\\n",
        "    \"diffusers>=0.15.0\"\n",
        "\n",
        "# Install proteinttt WITHOUT pulling its transitive deps (avoids numpy change)\n",
        "!pip install -q --no-deps \"proteinttt @ git+https://github.com/anton-bushuiev/ProteinTTT.git@e218659626214cc9aba302675f4285bb2ba6332c\"\n",
        "\n",
        "# â”€â”€ Patch fair-esm for Python 3.12 (mutable dataclass defaults) â”€â”€\n",
        "import importlib, importlib.util, re, sys, os\n",
        "\n",
        "import glob as _glob\n",
        "_esm_spec = importlib.util.find_spec(\"esm\")\n",
        "if _esm_spec and _esm_spec.origin:\n",
        "    _esm_v1_dir = os.path.join(\n",
        "        os.path.dirname(_esm_spec.origin), \"esmfold\", \"v1\"\n",
        "    )\n",
        "    # Patch ALL .py files under esm/esmfold/v1/ that have mutable dataclass defaults\n",
        "    for _fp in _glob.glob(os.path.join(_esm_v1_dir, \"*.py\")):\n",
        "        with open(_fp) as _f:\n",
        "            _src = _f.read()\n",
        "        if \"default_factory\" in _src or \"@dataclass\" not in _src:\n",
        "            continue  # already patched or no dataclasses to fix\n",
        "        print(f\"â³ Patching {_fp} for Python 3.12 dataclass compatibilityâ€¦\")\n",
        "        # Add 'from dataclasses import field' if missing\n",
        "        if \"from dataclasses import field\" not in _src:\n",
        "            _src = _src.replace(\n",
        "                \"from dataclasses import dataclass\",\n",
        "                \"from dataclasses import dataclass, field\",\n",
        "            )\n",
        "            # If file uses dataclass but doesn't import it directly (uses full path)\n",
        "            if \"from dataclasses import\" not in _src:\n",
        "                _src = \"from dataclasses import field\\n\" + _src\n",
        "        # Replace mutable defaults:  = SomeConfig()  â†’  = field(default_factory=SomeConfig)\n",
        "        # Handles both `: SomeConfig = SomeConfig()` and `: T.Any = SomeConfig()` patterns\n",
        "        _src = re.sub(\n",
        "            r\"(:\\s*\\S+)\\s*=\\s*(\\w+Config)\\(\\)\",\n",
        "            r\"\\1 = field(default_factory=\\2)\",\n",
        "            _src,\n",
        "        )\n",
        "        with open(_fp, \"w\") as _f:\n",
        "            _f.write(_src)\n",
        "        print(f\"  âœ… Patched {os.path.basename(_fp)}\")\n",
        "    print(\"  âœ… esm/esmfold/v1/ dataclass patches complete.\")\n",
        "else:\n",
        "    print(\"  âš ï¸ esm package not found â€” skipping dataclass patches\")\n",
        "\n",
        "# Verify critical packages imported correctly\n",
        "import pytorch_lightning, torchmetrics, transformers\n",
        "print(f\"âœ… pip packages installed \"\n",
        "      f\"(pytorch-lightning {pytorch_lightning.__version__}, \"\n",
        "      f\"transformers {transformers.__version__}).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpUeKz7OYvh3"
      },
      "source": [
        "## 3 Â· Install openfold & lora-diffusion (patched builds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WR_BsvenYvh3",
        "outputId": "62eb28b5-10e2-4703-e435-7a55503ae375"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… openfold already installed.\n",
            "âœ… lora-diffusion already installed.\n"
          ]
        }
      ],
      "source": [
        "import importlib, importlib.util, os, re, shutil, subprocess, sys, tempfile\n",
        "\n",
        "def _pip_install(*args):\n",
        "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", *args]\n",
        "    print(f\"  â†’ {' '.join(cmd)}\")\n",
        "    subprocess.check_call(cmd)\n",
        "    importlib.invalidate_caches()\n",
        "\n",
        "# â”€â”€ openfold â”€â”€\n",
        "try:\n",
        "    import openfold\n",
        "    print(\"âœ… openfold already installed.\")\n",
        "except ImportError:\n",
        "    print(\"â³ Installing openfold (may take a few minutes)â€¦\")\n",
        "    _pip_install(\"--no-build-isolation\",\n",
        "                 \"dllogger @ git+https://github.com/NVIDIA/dllogger.git\")\n",
        "    _of_dir = os.path.join(tempfile.gettempdir(), \"_openfold_build\")\n",
        "    if os.path.exists(_of_dir):\n",
        "        shutil.rmtree(_of_dir)\n",
        "    subprocess.check_call([\"git\", \"clone\", \"--filter=blob:none\", \"-q\",\n",
        "                           \"https://github.com/aqlaboratory/openfold.git\", _of_dir])\n",
        "    subprocess.check_call([\"git\", \"checkout\", \"-q\",\n",
        "                           \"4b41059694619831a7db195b7e0988fc4ff3a307\"], cwd=_of_dir)\n",
        "    _sp = os.path.join(_of_dir, \"setup.py\")\n",
        "    with open(_sp) as f:\n",
        "        _src = f.read()\n",
        "    _src = _src.replace(\"from scripts.utils import get_nvidia_cc\",\n",
        "                         \"# from scripts.utils import get_nvidia_cc  # patched\")\n",
        "    _src = re.sub(r\"compute_capabilities\\s*=\\s*set\\(\\[.*?set\\(\\[compute_capability\\]\\)\",\n",
        "                  \"compute_capabilities = set([(7, 0), (8, 0), (8, 6), (9, 0)])\",\n",
        "                  _src, flags=re.DOTALL)\n",
        "    _src = _src.replace(\"-std=c++14\", \"-std=c++17\")\n",
        "    with open(_sp, \"w\") as f:\n",
        "        f.write(_src)\n",
        "    os.environ[\"TORCH_CUDA_ARCH_LIST\"] = \"7.0;8.0;8.6;9.0\"\n",
        "    _pip_install(\"--no-build-isolation\", _of_dir)\n",
        "    shutil.rmtree(_of_dir, ignore_errors=True)\n",
        "\n",
        "    # â”€â”€ Patch openfold for pytorch-lightning â‰¥ 2.0 â”€â”€\n",
        "    # seed_everything moved from pytorch_lightning.utilities.seed â†’ top-level\n",
        "    _of_seed = os.path.join(\n",
        "        os.path.dirname(importlib.util.find_spec(\"openfold\").origin),\n",
        "        \"utils\", \"seed.py\",\n",
        "    )\n",
        "    if os.path.isfile(_of_seed):\n",
        "        with open(_of_seed) as _f:\n",
        "            _seed_src = _f.read()\n",
        "        if \"pytorch_lightning.utilities.seed\" in _seed_src:\n",
        "            _seed_src = _seed_src.replace(\n",
        "                \"from pytorch_lightning.utilities.seed import seed_everything\",\n",
        "                \"from pytorch_lightning import seed_everything\",\n",
        "            )\n",
        "            with open(_of_seed, \"w\") as _f:\n",
        "                _f.write(_seed_src)\n",
        "            print(\"  âœ… Patched openfold/utils/seed.py for pytorch-lightning â‰¥ 2.0\")\n",
        "\n",
        "    import openfold\n",
        "    print(\"âœ… openfold installed.\")\n",
        "\n",
        "# â”€â”€ lora-diffusion â”€â”€\n",
        "if importlib.util.find_spec(\"lora_diffusion\") is None:\n",
        "    print(\"â³ Installing lora-diffusionâ€¦\")\n",
        "    _pip_install(\"--no-deps\", \"--no-build-isolation\",\n",
        "                 \"lora-diffusion @ git+https://github.com/cloneofsimo/lora.git\")\n",
        "    _spec = importlib.util.find_spec(\"lora_diffusion\")\n",
        "    _init = os.path.join(os.path.dirname(_spec.origin), \"__init__.py\")\n",
        "    with open(_init, \"w\") as f:\n",
        "        f.write(\"from .lora import *\\n\")\n",
        "    from lora_diffusion.lora import inject_trainable_lora as _chk\n",
        "    del _chk\n",
        "    print(\"âœ… lora-diffusion installed.\")\n",
        "else:\n",
        "    print(\"âœ… lora-diffusion already installed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSx3SlhXYvh4"
      },
      "source": [
        "## 4 Â· Load ESMFold model onto GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "p3H-uvGbYvh4",
        "outputId": "054d7f3d-1648-4cb7-b6cf-edbefee13042"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (no files needed patching)\n",
            "â³ Loading ESMFold modelâ€¦\n"
          ]
        }
      ],
      "source": [
        "import sys, importlib, glob, re, torch\n",
        "\n",
        "# â”€â”€ Patch ALL esm dataclass files for Python 3.12 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Python 3.12 rejects mutable defaults in dataclasses; fair-esm uses them\n",
        "# extensively. We patch every .py under esm/esmfold/ that has this pattern.\n",
        "\n",
        "_esm_roots = glob.glob(f\"{sys.prefix}/lib/python*/dist-packages/esm/esmfold\") + \\\n",
        "             glob.glob(f\"{sys.prefix}/lib/python*/site-packages/esm/esmfold\")\n",
        "\n",
        "_patched = 0\n",
        "for _root in _esm_roots:\n",
        "    import os\n",
        "    for _dirpath, _, _fnames in os.walk(_root):\n",
        "        for _fname in _fnames:\n",
        "            if not _fname.endswith(\".py\"):\n",
        "                continue\n",
        "            _fp = os.path.join(_dirpath, _fname)\n",
        "            with open(_fp) as _f:\n",
        "                _src = _f.read()\n",
        "\n",
        "            _new = _src\n",
        "            # Ensure 'field' is imported from dataclasses\n",
        "            if \"from dataclasses import dataclass\" in _new and \"field\" not in _new:\n",
        "                _new = _new.replace(\n",
        "                    \"from dataclasses import dataclass\",\n",
        "                    \"from dataclasses import dataclass, field\",\n",
        "                )\n",
        "            # Replace   : SomeType = SomeType(...)   â†’   : SomeType = field(default_factory=SomeType)\n",
        "            # Handles zero-arg and any-arg constructors\n",
        "            _new = re.sub(\n",
        "                r\"(:\\s*)(\\w+)\\s*=\\s*\\2\\([^)]*\\)\",\n",
        "                r\"\\1\\2 = field(default_factory=\\2)\",\n",
        "                _new,\n",
        "            )\n",
        "\n",
        "            if _new != _src:\n",
        "                with open(_fp, \"w\") as _f:\n",
        "                    _f.write(_new)\n",
        "                _patched += 1\n",
        "                print(f\"  Patched: {_fp}\")\n",
        "\n",
        "if _patched:\n",
        "    print(f\"âœ… Patched {_patched} file(s) for Python 3.12 dataclass compat.\")\n",
        "else:\n",
        "    print(\"  (no files needed patching)\")\n",
        "\n",
        "# Flush ALL cached esm modules so patched files are loaded fresh\n",
        "_to_remove = [k for k in list(sys.modules.keys()) if k == \"esm\" or k.startswith(\"esm.\")]\n",
        "for _k in _to_remove:\n",
        "    del sys.modules[_k]\n",
        "\n",
        "import esm\n",
        "\n",
        "print(\"â³ Loading ESMFold modelâ€¦\")\n",
        "base_model = esm.pretrained.esmfold_v1()\n",
        "base_model = base_model.eval().to(DEVICE)\n",
        "base_model.set_chunk_size(128)\n",
        "print(f\"âœ… ESMFold loaded on {DEVICE}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh3JvB28Yvh4"
      },
      "source": [
        "## 5 Â· Baseline ESMFold prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JR05W3LcYvh4"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "t0 = time.time()\n",
        "print(\"â³ Running ESMFold baseline predictionâ€¦\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    baseline_output = base_model.infer(SEQUENCE)\n",
        "\n",
        "baseline_pdb   = base_model.output_to_pdb(baseline_output)[0]\n",
        "baseline_plddt = baseline_output[\"mean_plddt\"].item()\n",
        "\n",
        "print(f\"âœ… Baseline pLDDT: {baseline_plddt:.2f}  ({time.time()-t0:.1f}s)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6huA9qSlYvh4"
      },
      "source": [
        "## 6 Â· ProteinTTT â€” test-time training + prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1S6uiA7cYvh4"
      },
      "outputs": [],
      "source": [
        "from proteinttt.models.esmfold import ESMFoldTTT, DEFAULT_ESMFOLD_TTT_CFG\n",
        "\n",
        "# Configure TTT\n",
        "ttt_cfg = DEFAULT_ESMFOLD_TTT_CFG\n",
        "ttt_cfg.steps      = TTT_STEPS\n",
        "ttt_cfg.lr         = LEARNING_RATE\n",
        "ttt_cfg.batch_size = BATCH_SIZE\n",
        "ttt_cfg.lora_rank  = LORA_RANK\n",
        "ttt_cfg.lora_alpha = LORA_ALPHA\n",
        "ttt_cfg.seed       = 0\n",
        "ttt_cfg.eval_each_step = True   # avoid upstream UnboundLocalError\n",
        "\n",
        "ttt_model = ESMFoldTTT.ttt_from_pretrained(\n",
        "    base_model,\n",
        "    ttt_cfg=ttt_cfg,\n",
        "    esmfold_config=base_model.cfg,\n",
        ")\n",
        "\n",
        "# â”€â”€ Run test-time training â”€â”€\n",
        "t0 = time.time()\n",
        "print(f\"â³ Running ProteinTTT ({TTT_STEPS} steps, lr={LEARNING_RATE}, batch={BATCH_SIZE}, \"\n",
        "      f\"LoRA r={LORA_RANK}/Î±={LORA_ALPHA})â€¦\")\n",
        "\n",
        "ttt_result = ttt_model.ttt(SEQUENCE)\n",
        "\n",
        "print(f\"  TTT finished in {time.time()-t0:.1f}s\")\n",
        "\n",
        "# â”€â”€ Run prediction with TTT-adapted model â”€â”€\n",
        "print(\"â³ Running ESMFold+ProteinTTT predictionâ€¦\")\n",
        "with torch.no_grad():\n",
        "    ttt_output = ttt_model.infer(SEQUENCE)\n",
        "\n",
        "ttt_pdb   = ttt_model.output_to_pdb(ttt_output)[0]\n",
        "ttt_plddt = ttt_output[\"mean_plddt\"].item()\n",
        "\n",
        "print(f\"âœ… ProteinTTT pLDDT: {ttt_plddt:.2f}\")\n",
        "\n",
        "# Clean up TTT state\n",
        "ttt_model.ttt_reset()\n",
        "del ttt_model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0bHbO_ZYvh5"
      },
      "source": [
        "## 7 Â· Save PDB files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwbRT9zbYvh5"
      },
      "outputs": [],
      "source": [
        "baseline_pdb_path = os.path.join(OUTPUT_DIR, \"esmfold_baseline.pdb\")\n",
        "ttt_pdb_path      = os.path.join(OUTPUT_DIR, \"esmfold_proteinttt.pdb\")\n",
        "\n",
        "with open(baseline_pdb_path, \"w\") as f:\n",
        "    f.write(baseline_pdb)\n",
        "with open(ttt_pdb_path, \"w\") as f:\n",
        "    f.write(ttt_pdb)\n",
        "\n",
        "print(f\"âœ… Baseline PDB saved â†’ {baseline_pdb_path}\")\n",
        "print(f\"âœ… TTT PDB saved      â†’ {ttt_pdb_path}\")\n",
        "\n",
        "# Offer download in Colab\n",
        "try:\n",
        "    from google.colab import files as colab_files\n",
        "    print(\"\\nðŸ“¥ Click below to download the PDB files:\")\n",
        "    colab_files.download(baseline_pdb_path)\n",
        "    colab_files.download(ttt_pdb_path)\n",
        "except ImportError:\n",
        "    print(\"(Not running in Colab â€” files saved to disk.)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1pQCBVMYvh5"
      },
      "source": [
        "## 8 Â· Results & 3D Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glW2sth5Yvh5"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, HTML\n",
        "import html as html_lib, base64\n",
        "\n",
        "# â”€â”€ Summary table â”€â”€\n",
        "improvement = ttt_plddt / baseline_plddt if baseline_plddt > 0 else 0\n",
        "\n",
        "summary_html = f\"\"\"\n",
        "<div style=\"font-family: 'Helvetica Neue', Arial, sans-serif; max-width: 700px;\">\n",
        "  <h2 style=\"margin-bottom: 4px;\">Results</h2>\n",
        "  <table style=\"border-collapse: collapse; width: 100%; font-size: 15px;\">\n",
        "    <tr style=\"background: #f7f7f7;\">\n",
        "      <th style=\"text-align:left; padding:8px 12px; border-bottom:2px solid #ddd;\">Metric</th>\n",
        "      <th style=\"text-align:right; padding:8px 12px; border-bottom:2px solid #ddd;\">Value</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td style=\"padding:8px 12px; border-bottom:1px solid #eee;\">Sequence length</td>\n",
        "      <td style=\"padding:8px 12px; border-bottom:1px solid #eee; text-align:right;\">{len(SEQUENCE)} residues</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td style=\"padding:8px 12px; border-bottom:1px solid #eee;\">ESMFold pLDDT</td>\n",
        "      <td style=\"padding:8px 12px; border-bottom:1px solid #eee; text-align:right; font-weight:600;\">{baseline_plddt:.2f}</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td style=\"padding:8px 12px; border-bottom:1px solid #eee;\">ESMFold + ProteinTTT pLDDT</td>\n",
        "      <td style=\"padding:8px 12px; border-bottom:1px solid #eee; text-align:right; font-weight:600; color:#16a34a;\">{ttt_plddt:.2f}</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td style=\"padding:8px 12px; border-bottom:1px solid #eee;\">Improvement ratio</td>\n",
        "      <td style=\"padding:8px 12px; border-bottom:1px solid #eee; text-align:right; font-weight:600;\">{improvement:.2f}x</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td style=\"padding:8px 12px;\">TTT config</td>\n",
        "      <td style=\"padding:8px 12px; text-align:right; font-size:13px;\">steps={TTT_STEPS}, lr={LEARNING_RATE}, batch={BATCH_SIZE}, LoRA r={LORA_RANK}/Î±={LORA_ALPHA}</td>\n",
        "    </tr>\n",
        "  </table>\n",
        "</div>\n",
        "\"\"\"\n",
        "display(HTML(summary_html))\n",
        "\n",
        "\n",
        "# â”€â”€ 3Dmol.js viewer builder â”€â”€\n",
        "def _mol_viewer(pdb_content, title):\n",
        "    pdb_esc = pdb_content.replace(\"\\\\\", \"\\\\\\\\\").replace(\"`\", \"\\\\`\").replace(\"$\", \"\\\\$\")\n",
        "    title_esc = html_lib.escape(title)\n",
        "    h = f\"\"\"\n",
        "    <div style=\"font-family:sans-serif; font-weight:bold; text-align:center;\n",
        "                padding:6px; background:#f0f0f0; font-size:14px;\">{title_esc}</div>\n",
        "    <div id=\"{id(pdb_content)}\" style=\"width:100%; height:450px; position:relative;\">\n",
        "      <div style=\"position:absolute;right:10px;bottom:10px;z-index:10;background:rgba(255,255,255,.9);\n",
        "                  border:1px solid #d9d9d9;border-radius:8px;padding:8px 10px;font-size:12px;\n",
        "                  box-shadow:0 1px 4px rgba(0,0,0,.15);line-height:1.4;\">\n",
        "        <b>pLDDT</b><br>\n",
        "        <span style=\"color:#0d57d3;\">â– </span> â‰¥90 &nbsp;\n",
        "        <span style=\"color:#6acbf1;\">â– </span> 70â€“90 &nbsp;\n",
        "        <span style=\"color:#fed936;\">â– </span> 50â€“70 &nbsp;\n",
        "        <span style=\"color:#fd7d4d;\">â– </span> &lt;50\n",
        "      </div>\n",
        "    </div>\n",
        "    <script src=\"https://3Dmol.csb.pitt.edu/build/3Dmol-min.js\"></script>\n",
        "    <script>\n",
        "      (function() {{\n",
        "        var v = $3Dmol.createViewer(\"{id(pdb_content)}\", {{backgroundColor:\"white\"}});\n",
        "        v.addModel(`{pdb_esc}`, \"pdb\");\n",
        "        v.setStyle({{}}, {{cartoon:{{colorfunc:function(a){{\n",
        "          if(a.b<50) return \"#fd7d4d\";\n",
        "          if(a.b<70) return \"#fed936\";\n",
        "          if(a.b<90) return \"#6acbf1\";\n",
        "          return \"#0d57d3\";\n",
        "        }}}}}});\n",
        "        v.zoomTo(); v.render();\n",
        "      }})();\n",
        "    </script>\"\"\"\n",
        "    return h\n",
        "\n",
        "\n",
        "print(\"â”€\" * 60)\n",
        "print(\"ESMFold  (baseline)\")\n",
        "print(\"â”€\" * 60)\n",
        "display(HTML(_mol_viewer(baseline_pdb, f\"ESMFold  â€”  pLDDT {baseline_plddt:.1f}\")))\n",
        "\n",
        "print()\n",
        "print(\"â”€\" * 60)\n",
        "print(\"ESMFold + ProteinTTT\")\n",
        "print(\"â”€\" * 60)\n",
        "display(HTML(_mol_viewer(ttt_pdb, f\"ESMFold + ProteinTTT  â€”  pLDDT {ttt_plddt:.1f}\")))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
